---
layout: post
title: DirtyPipe-CVE-2022-0847
author: 0xnull007
tags:
  - DirtyPipe
  - CVE-2022-0847
comments: true
permalink: /posts/dirtypipe-cve-2022-0847
description: Analyzed and wrote PoC for CVE-2022-0847 aka DirtyPipe
---
My colleague `stdnoerr` wrote a blog about his N-day research on DirtyPipe (CVE-2022-0847). Being a noob in kernel exploitation, I realized that, I should be familiar with some Linux kernel internals to understand his blog fully. So, I decided to explore those internals and write about my journey so other persons like me could benefit from it. This posts will cover only those internals which will be necessary to understand the dirt pipe vulnerability and its exploitation. We will go through all the important kernel structures in a sequence and then merge them all at the end to get the whole story.

## Pipe

The first and the most important kernel concept/structure involved in this vulnerability is a `pipe`.  A `pipe` is a unidirectional inter-process communication (IPC) mechanism found in UNIX-like operating systems.  In essence, a pipe is a buffer in kernel space that is accessed by processes through file descriptors. You might have used it in your shell commands like:

```bash
cat /proc/cpuinfo | grep "address size"
```

Here, the `|` operator creates a pipe (a buffer in kernel space). The output of `cat` is written into this pipe, and the input of `grep` is read from the same pipe. Such a pipe can be created programmatically using the syscall [pipe()](https://linux.die.net/man/2/pipe) and we get 2 file descriptors, one is the reading end while the other is writing end of that pipe.  

In Linux, every file is represented by a special data structure called an [inode](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/fs.h#L620), which stores important information about the file (like its type, size, and permissions). Pipes in the Linux kernel are built on top of a virtual filesystem (VFS). When you create a pipe, the two file descriptors you get point to two pseudo files having different permissions — one is read-only, and the other is write-only but both of these files have a single inode. This inode  has a field called [i_pipe](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/fs.h#L708), which [points](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L881) to a kernel structure named [`pipe_inode_info`](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/pipe_fs_i.h#L58). This structure is what the kernel uses to manage the actual metadata of a pipe.

### Key Data Structures

1. [**`struct pipe_inode_info`**](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/pipe_fs_i.h#L58)
    - Tracks read/write positions, buffers, and synchronization.
    - **`bufs`:** an array of **`struct pipe_buffer`**, each representing a memory page storing pipe data.
    - **`ring_size`:** size of the array `bufs`.
2. [**`struct pipe_buffer`**](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/pipe_fs_i.h#L26)
    - **`page`:** pointer to `struct page` describing where the actual data held by the `pipe_buffer` is stored.
    - **`offset`**, **`len`:** Track where valid data exists in the page.
    - **`ops`:** Operations table (`pipe_buf_operations`) for managing the buffer.    
### Operations on a Pipe

#### Pipe Creation (`pipe()`)

1. **`pipe()/pipe2()` syscall** → [`do_pipe2()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L1000)` → `__do_pipe_flags()    
2. Allocates a `struct pipe_inode_info` via [`alloc_pipe_info()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L788).
3. Creates two file descriptors (read & write ends) via [`get_unused_fd_flags()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L962).
4. Initializes 16 pipe buffers (default) as [`PIPE_DEF_BUFFERS`](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/pipe_fs_i.h#L5). Note that each `pipe_buffer` has one page associated with it, which means that the total capacity of the pipe is `ring_size * 4096` bytes. A process can get and set the size of this ring using the `fcntl()` system call with the `F_GETPIPE_SZ` and `F_SETPIPE_SZ` flags, respectively.
	 - `ring_size` is always the power of 2. Means, if we set it to 3, kernel automatically will round off it to the next even number. 
#### Writing to a Pipe (`write()`)

1. **`write()` syscall** → [`vfs_write()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/read_write.c#L570) → [`pipe_write()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L414).
2. If the pipe is full, the writer sleeps until space is available.
3. Kernel [allocates](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L494) a page (if needed) and copies data from user space.
4. Updates `pipe_buffer`'s [`offset`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L522), `len`, and `flags`.
#### **Reading from a Pipe (`read()`)**

1. **`read()` syscall** → `vfs_read()` → [`pipe_read()`](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L230).
2. If the pipe is empty, the reader sleeps until data arrives.
3. Kernel copies data from the `pipe_buffer` page to user space.
4. If the buffer is fully consumed, the page is ['freed'](https://elixir.bootlin.com/linux/v5.16.10/source/fs/pipe.c#L322) or marked for reuse.

The array  `bufs` in `struct pipe_inode_info` is a circular array (or ring buffer):
- It has a fixed size (defined by `ring_size` in `pipe_inode_info`).
- It uses two pointers (`head` and `tail`) to track where new data is written (`head`) and where data is read (`tail`).
- New data is written to `bufs[head % (ring_size - 1)]` and  `head` is incremented. As `ring_size` is always a power of 2, when `head` reaches `ring_size`, `head % (ring_size - 1)` wraps around to `0` (hence "circular").
-  When `head - tail == ring_size`, the pipe is full, new writes either wait (block) or overwrite old data (depending on configuration).
- When `head == tail`,  the buffer is empty, reads block until new data arrives. Here is pictorial view of whatever is discussed so far.

![pipe_inode_info.svg](/assets/images/posts/2025-08-26-DirtyPipe-CVE-2022-0847/pipe_inode_info.svg)

## **Page Cache**

The page cache plays an important role in the Dirty Pipe vulnerability so let’s see what it is and how it works. The page cache is a kernel-managed memory region that stores recently accessed file data and disk blocks in RAM. It can be considered as caching layer for the file I/O to speed it up.  
According to Linux kernel's documentation:
> The physical memory is volatile and the common case for getting data into the memory is to read it from files. Whenever a file is read, the data is put into the page cache to avoid expensive disk access on the subsequent reads. Similarly, when one writes to a file, the data is placed in the page cache and eventually gets into the backing storage device. The written pages are marked as dirty and when Linux decides to reuse them for other purposes, it makes sure to synchronize the file contents on the device with the updated data. [source](https://www.kernel.org/doc/html/latest/admin-guide/mm/concepts.html#page-cache)

Kernel doesn't just stores the recently accessed file data into page cache but it uses an optimization mechanism called `read-ahead`  which notices the access pattern, guesses pages you’ll need next and loads them into memory in advance. So, If you are reading a file in a sequence, kernel will load the rest of the pages of that file into memory as well.

Due to this caching layer,  if any process on the system (or the kernel itself) requests data from a file that is already cached, the cached data is used instead of accessing the disk.  This default behavior  can be changed by using flags (`O_DIRECT | O_SYNC`) when opening a file. However, in most situations the cached data is what is ultimately used by the kernel (and thus also the user processes).

Whenever a file is opened, kernel stores its metadata into [struct inode](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/fs.h#L620) . Among that metadata, there is a field named [`i_mapping`](https://elixir.bootlin.com/linux/v5.16.10/source/include/linux/fs.h#L634)  of type `struct address_space` which contains an array of pointers pointing to the pages in  _page cache_  to which that file is mapped.  

![page_cache.svg](/assets/images/posts/2025-08-26-DirtyPipe-CVE-2022-0847/page_cache.svg)

## **`splice()` syscall**

`splice` syscall is a member of zero-copy syscalls in Linux kernel. Zero-copy syscalls are system calls that allow data to be transferred between kernel objects (such as files, sockets, and pipes) without copying the data into or out of user space memory. Let's make it clear by taking a scenario where we have to copy the contents of a file into a pipe. The naive approach will be to `open`  and `read` contents from that file into a user buffer and then `write` that buffer's contents into a pipe.  The following diagrams shows the steps involved in above approach.

```C
			     Reading data from a file
________________________________________________________
[Disk/File]-->[Kernel buffer/page cache]-->[User buffer]-->[pipe/kernel buffer]
									       ____________________________________
											        Writing into a pipe
```

We can see, to copy the data from a file into a pipe, we had to copy it into user-space buffer first which is a redundant and costly. `splice` syscall eliminates this step by reusing the page cache where that file is cached. Instead of copying the data from page cache to user buffer/userland, It [copies](https://elixir.bootlin.com/linux/v5.16.10/source/lib/iov_iter.c#L420) the address of page cache into the `page` pointer of the `pipe_buffer`.  The following diagram illustrates it:

![pipe_page_cache.svg](/assets/images/posts/2025-08-26-DirtyPipe-CVE-2022-0847/pipe_page_cache.svg)

Let's see what does the manpage of `splice` syscall say:
```

SPLICE(2)                       Linux Programmer's Manual                      

NAME
       splice - splice data to/from a pipe

SYNOPSIS
       #define _GNU_SOURCE         /* See feature_test_macros(7) */
       #include <fcntl.h>
       ssize_t splice(int fd_in, off64_t *off_in, int fd_out,
                      off64_t *off_out, size_t len, unsigned int flags);
DESCRIPTION
       splice()  moves  data between two file descriptors without copying between kernel address space and user address space.  It transfers up to len bytes of data from the file descriptor fd_in to the file descriptor fd_out, where one of the file descriptors must refer to a pipe.
The following semantics apply for fd_in and off_in:
	* If fd_in refers to a pipe, then off_in must be NULL.
    * If fd_in does not refer to a pipe and off_in is NULL, then bytes are read        from fd_in starting from the file offset, and the file offset is adjusted        appropriately.
    * If fd_in does not refer to a pipe and off_in is not  NULL,  then  off_in         must point to a buffer which specifies the starting offset from which bytes will be read from fd_in; in this case, the file offset of fd_in is not changed.
Analogous statements apply for fd_out and off_out.
```

Let's take a simple example code to understand `splice()`.

```c
#define _GNU_SOURCE
#include <fcntl.h>
#include <stdio.h>
#include <unistd.h>
#include <string.h>

#define TARGET_FILE "./f1"

int main() {
    int fd;
    int pipefd[2];
    char buffer[256];

    // 1. Create pipe and open target file
    if (pipe(pipefd) == -1) {
        perror("pipe");
        return 1;
    }

    if ((fd = open(TARGET_FILE, O_RDONLY)) == -1) {
            perror("open");
            return 1;
    }
    
    // 2. Splice the file
    if (splice(fd, NULL, pipefd[1], NULL, sizeof(buffer), 0) < 0) {
        perror("splice");
        close(fd);
        close(pipefd[0]);
        close(pipefd[1]);
        return 1;
    }
   
    read(pipefd[0], buffer, sizeof(buffer));
    printf("Data read from target file: %s\n", buffer);

    return 0;
}
```

The above code snippet opens a file `f1` and `splice`s it into the pipe, and then we can performs a read operation  on pipe to read the file contents.

## Vulnerability

As discussed earlier, due to page cache, whenever a process accesses a file, the contents from page cache are accessed if that file is already cached there even by another process. Let's take a scenario of two process `A` and `B`. Process `A` first accessed a file `f1.txt` which means it is cached and its owner is process `A` . Later process `B` splices that same file, as that file was already cached, so the pages whose owner is process `A` are mapped in process `B`'s address space and now the process `B` can write on those pages as well.   
 